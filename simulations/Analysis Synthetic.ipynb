{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from generate import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../auton-survival/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Results_ntc/' # Path where the data is saved\n",
    "mode =  'obs' # Mode to be observational or randomised\n",
    "random_seed = 42 # Critical to ensure the data is the same\n",
    "parameter = 30000 # Parameter being tested\n",
    "root = 'generatesize_' # generatelinear_ or generate_ or generatesmall_ or generatehomogenous_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycox.evaluation import EvalSurv\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "### Utils: The evaluatino metrics used\n",
    "def evaluate(clusters_pred, te_cluster, survival_pred, survival_gt, a, t, e, times, groups): \n",
    "    folds = survival_pred[('Use',)]\n",
    "    survival_pred = survival_pred.drop(columns = ['Use', 'Assignment'])\n",
    "    results = {}\n",
    "\n",
    "    # Compute performance for each fold\n",
    "    for fold in np.arange(5):\n",
    "        # Subselect all variables in fold\n",
    "        clusters_pred_fold, survival_pred_fold = clusters_pred[(folds == fold).values], survival_pred[(folds == fold).values]\n",
    "        groups_fold = groups.loc[clusters_pred_fold.index]\n",
    "        te_fold, te_gt_fold = survival_pred_fold['treated'] - survival_pred_fold['untreated'], \\\n",
    "                        (survival_gt['treated'] - survival_gt['untreated']).loc[survival_pred_fold.index]\n",
    "\n",
    "        # Evaluate quality cluster at the population level\n",
    "        results_fold = {}\n",
    "        results_fold['Population'] = {\n",
    "            \"Rand_index\": adjusted_rand_score(groups_fold, clusters_pred_fold.apply(lambda x: x.argmax(), 1)), \n",
    "            \"MSE_Mean_TE\": mse_mean(te_fold, te_gt_fold)}\n",
    "        \n",
    "        # At the group level\n",
    "        for group in groups.unique() if groups is not None else []:\n",
    "            selection = groups_fold == group\n",
    "            alpha_max = clusters_pred_fold[selection].mean(0).argmax()\n",
    "            cluster_te = te_cluster[fold][:, alpha_max] if te_cluster is not None else None\n",
    "            results_fold[group] = {\n",
    "                \"MSE_Cluster_TE\": mse_cluster(te_gt_fold[selection], cluster_te) if te_cluster is not None else np.nan\n",
    "            }\n",
    "\n",
    "        # Measure predicitive performance\n",
    "        results_fold = {\n",
    "            'Overall': pd.DataFrame(results_fold),\n",
    "            'Factual': pd.DataFrame(performance_metric(survival_pred_fold, survival_gt, a == 1, t, e, times, groups))}\n",
    "        results[fold] = pd.concat(results_fold, axis = 1)\n",
    "\n",
    "    return pd.concat(results)\n",
    "\n",
    "def mse_mean(pred, gt):\n",
    "    return np.abs(pred.mean(0).values - gt.mean(0).values).mean()\n",
    "\n",
    "def mse_cluster(pred, mean):\n",
    "    return np.abs(pred.mean(0).values - mean).mean()\n",
    "\n",
    "def performance_metric(survival_pred, survival_gt, a, t, e, times, groups):\n",
    "    train_index = survival_gt.index.difference(survival_pred.index)\n",
    "    \n",
    "    # Select data\n",
    "    survival_pred = pd.concat([survival_pred['treated'][a.loc[survival_pred.index]], survival_pred['untreated'][~a.loc[survival_pred.index]]], axis = 0).loc[survival_pred.index]\n",
    "    survival_gt = pd.concat([survival_gt['treated'][a], survival_gt['untreated'][~a]], axis = 0).loc[survival_gt.index]\n",
    "    survival_pred.columns = survival_pred.columns.astype(float) \n",
    "    survival_gt.columns = survival_gt.columns.astype(float) \n",
    "\n",
    "    # Evaluate\n",
    "    e_train, t_train = e.loc[train_index].values, t.loc[train_index].values\n",
    "    e_test,  t_test  = e.loc[survival_pred.index].values, t.loc[survival_pred.index].values\n",
    "    g_test = groups.loc[survival_pred.index]\n",
    "\n",
    "    selection = (t_test < t_train.max()) | (e_test == 0)\n",
    "    survival_pred = survival_pred[selection]\n",
    "    e_test, t_test, g_test = e_test[selection], t_test[selection], g_test[selection]\n",
    "\n",
    "    survival_train = survival_gt.loc[train_index]\n",
    "\n",
    "    km = EvalSurv(survival_train.T, t_train, e_train, censor_surv = 'km')\n",
    "    test_eval = EvalSurv(survival_pred.T, t_test, e_test, censor_surv = km)\n",
    "    results = {'Population':{}} \n",
    "    try: results['Population']['CIS'] = test_eval.concordance_td()\n",
    "    except: results['Population']['CIS'] = np.nan\n",
    "    try: results['Population']['BRS'] = test_eval.integrated_brier_score(times.to_numpy())\n",
    "    except: results['Population']['BRS'] = np.nan\n",
    "\n",
    "    for group in groups.unique() if groups is not None else []:\n",
    "        test_eval = EvalSurv(survival_pred[g_test == group].T, t_test[g_test == group], e_test[g_test == group], censor_surv = km)\n",
    "        results[group] = {}\n",
    "        try: results[group]['CIS'] = test_eval.concordance_td()\n",
    "        except: results[group]['CIS'] = np.nan\n",
    "        try: results[group]['BRS'] = test_eval.integrated_brier_score(times.to_numpy())\n",
    "        except: results[group]['BRS'] = np.nan\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "# TODO: Add your method in the list for nicer display\n",
    "dict_name = {'ntc': 'NTC', 'ntc+uncorrect': 'NTC (Unadjusted)', 'cmhe+g': 'CMHE (Treatment)', 'cmhe+k': 'CMHE (Survival)'} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file and compute performance\n",
    "treated, untreated, clusters, results, te_cluster = {}, {}, {}, {}, {}\n",
    "for file_name in sorted(os.listdir(path)):\n",
    "    if root in file_name and '.csv' in file_name: \n",
    "        if not (str(random_seed) in file_name): continue\n",
    "        if mode not in file_name: continue\n",
    "        if (('{}+'.format(random_seed) in file_name) or ('{}+'.format(mode) in file_name)) and not(('+{}='.format(parameter) in file_name) or ('+{}_'.format(parameter) in file_name)): continue\n",
    "        model = file_name\n",
    "\n",
    "        model = model[model.rindex('_') + 1: model.rindex('.')]\n",
    "        model = dict_name[model] if model in dict_name else model\n",
    "        print(\"Opening :\", file_name, ' - ', model, ' - ', random_seed, ' - ', mode)\n",
    "\n",
    "        if model not in results:\n",
    "            results[model], treated[model], untreated[model], clusters[model], te_cluster[model] = {}, {}, {}, {}, {}\n",
    "\n",
    "        predictions = pd.read_csv(path + file_name, header = [0, 1], index_col = 0).dropna()\n",
    "        treated[model][random_seed]  = predictions[['treated']].droplevel(0, axis = 1)\n",
    "        untreated[model][random_seed]= predictions[['untreated']].droplevel(0, axis = 1)\n",
    "        clusters[model][random_seed] = predictions[['Assignment']].droplevel(0, axis = 1)\n",
    "\n",
    "        # Remove last columns and change name column to flo\n",
    "        times = treated[model][random_seed].columns.astype(float)\n",
    "\n",
    "        # Generate associated ground truth\n",
    "        if parameter == 5:\n",
    "            centers = ([0, 2.25], [-2.25, -1], [2.25, -1], [-3, 3], [4, 4])\n",
    "        else:\n",
    "            centers = ([0, 2.25], [-2.25, -1], [2.25, -1])\n",
    "\n",
    "        if 'linear' in root:\n",
    "            x, a, t, e, (cluster_centers, parameters, outcomes, assignement) = generate_linear(random_seed, mode = mode)\n",
    "            cifs = compute_cif_linear(x, outcomes.cluster, cluster_centers, parameters, times)\n",
    "        else:\n",
    "            x, a, t, e, (cluster_centers, parameters, outcomes, assignement) = generate(random_seed, mode = mode, centers = centers, \n",
    "                                                                                        homogenous = 'homogenous' in root, \n",
    "                                                                                        proportions = [0.625, 0.25, 0.125] if 'small' in root else None, \n",
    "                                                                                        size = parameter if 'size' in root else 3000,\n",
    "                                                                                        percentage_treatment = parameter if 'treat' in root else 0.5)\n",
    "            cifs = compute_cif(x, outcomes.cluster, cluster_centers, parameters, times)\n",
    "\n",
    "        model_file = file_name.replace('.csv', '.pickle')\n",
    "        if os.path.isfile(path + model_file):\n",
    "            model_pickle = Experiment.load(path + model_file)\n",
    "            te_cluster[model][random_seed] = model_pickle.clusters(times)\n",
    "            try:\n",
    "                print(model_pickle.best_model[0].predict_propensity(x.values))\n",
    "            except Exception as er:\n",
    "                print(er)\n",
    "        else:\n",
    "            te_cluster[model][random_seed] = None\n",
    "\n",
    "\n",
    "        # Evaluate\n",
    "        results[model][random_seed] = evaluate(clusters[model][random_seed], te_cluster[model][random_seed], predictions, 1 - cifs,\n",
    "                                               a, t, e, times, outcomes.cluster)\n",
    "else:\n",
    "    te_cluster['GT'] = {random_seed: {}}\n",
    "    for fold in range(5): \n",
    "        index = (predictions[('Use',)] == fold).values\n",
    "        te_cluster['GT'][random_seed][fold] =  (cifs['untreated'] - cifs['treated']).loc[index].groupby(outcomes.cluster.loc[index]).mean(0).T.values\n",
    "                                                \n",
    "\n",
    "results = pd.concat({model: pd.concat(results[model], names = ['Seed']) for model in results})\n",
    "results.index.set_names(['Model', 'Fold', 'Metric'], level = [0, 2, 3], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby(['Model', 'Seed', 'Metric']).apply(lambda x:  pd.Series([\"{:.3f} ({:.3f})\".format(mean, std) for mean, std in zip(x.mean(), x.std())], index = x.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "survival",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1b50223f39b64c0c24545f474e3e7d2d3b4b121fe045100fc03a3926bb649af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
